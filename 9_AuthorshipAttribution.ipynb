{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authorship Attribution\n",
    "\n",
    "Authorship analysis is, predominately, a text mining task that aims to identify certain aspects about an author, based only on the content of their writings. This could include characteristics such as age, gender, or background. In the specific authorship attribution task, we aim to identify who out of a set of authors wrote a particular document. This is a classic case of a classification task. In many ways, authorship analysis tasks are performed using standard data mining methodologies, such as cross fold validation, feature extraction, and classification algorithms.\n",
    "\n",
    "In this chapter, we will use the problem of authorship attribution to piece together the parts of the data mining methodology we developed in the previous chapters. We identify the problem and discuss the background and knowledge of the problem. This lets us choose features to extract, which we will build a pipeline for achieving. We will test two different types of features: function words and character n-grams. Finally, we will perform an in-depth analysis of the results. We will work with a book dataset, and then a very messy real-world corpus of e-mails.\n",
    "\n",
    "The topics we will cover in this chapter are as follows:\n",
    "- Feature engineering and how the features differ based on application\n",
    "- Revisiting the bag-of-words model with a specific goal in mind\n",
    "- Feature types and the character n-grams model\n",
    "- Support vector machines\n",
    "- Cleaning up a messy dataset for data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "The data we will use for this chapter is a set of books from Project Gutenberg at www.gutenberg.org, which is a repository of public domain literature works.\n",
    "\n",
    "The books I used for these experiments come from a variety of authors:\n",
    "- Booth Tarkington (22 titles)\n",
    "- Charles Dickens (44 titles)\n",
    "- Edith Nesbit (10 titles)\n",
    "- Arthur Conan Doyle (51 titles)\n",
    "- Mark Twain (29 titles)\n",
    "- Sir Richard Francis Burton (11 titles)\n",
    "- Emile Gaboriau (10 titles)\n",
    "\n",
    "Overall, there are 177 documents from 7 authors, giving a significant amount of text to work with. A full list of the titles, along with download links and a script to automatically fetch them, is given in the code bundle. To download these books, we use the requests library to download the files into our data directory. First, set up the data directory and ensure the following code\n",
    "links to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "from io import BytesIO, StringIO\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 19.28MB\n",
      "Text#,Type,Issued,Title,Language,Authors,Subjects,LoCC,Bookshelves\n",
      "1,Text,1971-12-01,The Declaration of Independence of the United States of America,en,\"Jefferson, Thomas, 1743-1826\",\"United States -- History -- Revolution, 1775-1783 -- Sources; United States. Declaration of Independence\",E201; JK,Politics; American Revolutionary War; United States Law; Browsing: History - American; Browsing: His\n"
     ]
    }
   ],
   "source": [
    "GUTENBERG_CSV_URL = \"https://www.gutenberg.org/cache/epub/feeds/pg_catalog.csv.gz\"\n",
    "\n",
    "r = requests.get(GUTENBERG_CSV_URL)\n",
    "csv_text = r.content.decode(\"utf-8\")\n",
    "\n",
    "print(f\"Total size: {len(r.content) / 1024**2:0.2f}MB\")\n",
    "print(csv_text[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text#</th>\n",
       "      <th>Type</th>\n",
       "      <th>Issued</th>\n",
       "      <th>Title</th>\n",
       "      <th>Language</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Subjects</th>\n",
       "      <th>LoCC</th>\n",
       "      <th>Bookshelves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>1971-12-01</td>\n",
       "      <td>The Declaration of Independence of the United ...</td>\n",
       "      <td>en</td>\n",
       "      <td>Jefferson, Thomas, 1743-1826</td>\n",
       "      <td>United States -- History -- Revolution, 1775-1...</td>\n",
       "      <td>E201; JK</td>\n",
       "      <td>Politics; American Revolutionary War; United S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Text</td>\n",
       "      <td>1972-12-01</td>\n",
       "      <td>The United States Bill of Rights\\r\\nThe Ten Or...</td>\n",
       "      <td>en</td>\n",
       "      <td>United States</td>\n",
       "      <td>Civil rights -- United States -- Sources; Unit...</td>\n",
       "      <td>JK; KF</td>\n",
       "      <td>Politics; American Revolutionary War; United S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Text</td>\n",
       "      <td>1973-11-01</td>\n",
       "      <td>John F. Kennedy's Inaugural Address</td>\n",
       "      <td>en</td>\n",
       "      <td>Kennedy, John F. (John Fitzgerald), 1917-1963</td>\n",
       "      <td>United States -- Foreign relations -- 1961-196...</td>\n",
       "      <td>E838</td>\n",
       "      <td>Browsing: History - American; Browsing: Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Text</td>\n",
       "      <td>1973-11-01</td>\n",
       "      <td>Lincoln's Gettysburg Address\\r\\nGiven November...</td>\n",
       "      <td>en</td>\n",
       "      <td>Lincoln, Abraham, 1809-1865</td>\n",
       "      <td>Consecration of cemeteries -- Pennsylvania -- ...</td>\n",
       "      <td>E456</td>\n",
       "      <td>US Civil War; Browsing: History - American; Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Text</td>\n",
       "      <td>1975-12-01</td>\n",
       "      <td>The United States Constitution</td>\n",
       "      <td>en</td>\n",
       "      <td>United States</td>\n",
       "      <td>United States -- Politics and government -- 17...</td>\n",
       "      <td>JK; KF</td>\n",
       "      <td>United States; Politics; American Revolutionar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Text#  Type      Issued                                              Title  \\\n",
       "0     1  Text  1971-12-01  The Declaration of Independence of the United ...   \n",
       "1     2  Text  1972-12-01  The United States Bill of Rights\\r\\nThe Ten Or...   \n",
       "2     3  Text  1973-11-01                John F. Kennedy's Inaugural Address   \n",
       "3     4  Text  1973-11-01  Lincoln's Gettysburg Address\\r\\nGiven November...   \n",
       "4     5  Text  1975-12-01                     The United States Constitution   \n",
       "\n",
       "  Language                                        Authors  \\\n",
       "0       en                   Jefferson, Thomas, 1743-1826   \n",
       "1       en                                  United States   \n",
       "2       en  Kennedy, John F. (John Fitzgerald), 1917-1963   \n",
       "3       en                    Lincoln, Abraham, 1809-1865   \n",
       "4       en                                  United States   \n",
       "\n",
       "                                            Subjects      LoCC  \\\n",
       "0  United States -- History -- Revolution, 1775-1...  E201; JK   \n",
       "1  Civil rights -- United States -- Sources; Unit...    JK; KF   \n",
       "2  United States -- Foreign relations -- 1961-196...      E838   \n",
       "3  Consecration of cemeteries -- Pennsylvania -- ...      E456   \n",
       "4  United States -- Politics and government -- 17...    JK; KF   \n",
       "\n",
       "                                         Bookshelves  \n",
       "0  Politics; American Revolutionary War; United S...  \n",
       "1  Politics; American Revolutionary War; United S...  \n",
       "2   Browsing: History - American; Browsing: Politics  \n",
       "3  US Civil War; Browsing: History - American; Br...  \n",
       "4  United States; Politics; American Revolutionar...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_info = pd.DataFrame(csv.DictReader(StringIO(csv_text)))\n",
    "books_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Booth', 'Tarkington'),\n",
       " ('Charles', 'Dickens'),\n",
       " ('Edith', 'Nesbit'),\n",
       " ('Arthur', 'Conan', 'Doyle'),\n",
       " ('Mark', 'Twain'),\n",
       " ('Sir', 'Richard', 'Francis', 'Burton'),\n",
       " ('Emile', 'Gaboriau')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_info['Authors_parsed'] = books_info['Authors'].apply(lambda x:\\\n",
    "                                                        x.replace(',', '').\\\n",
    "                                                        replace('(', '').\\\n",
    "                                                        replace(')', ''))\n",
    "rel_authors = ['Booth Tarkington', 'Charles Dickens', 'Edith Nesbit', \n",
    "    'Arthur Conan Doyle', 'Mark Twain', 'Sir Richard Francis Burton', \n",
    "    'Emile Gaboriau']\n",
    "\n",
    "authors = [tuple(authors.split()) for authors in rel_authors]\n",
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jefferson', 'Thomas', '1743-1826'), ('United', 'States')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_to_check = books_info['Authors_parsed'].unique()\n",
    "authors_to_check = [tuple(authors.split()) for authors in authors_to_check]\n",
    "authors_to_check[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "authors_to_keep = defaultdict(set)\n",
    "authors_to_keep_for_df = []\n",
    "for author_to_check in authors_to_check:\n",
    "    for author in authors:\n",
    "        if set(author).issubset(set(author_to_check)):\n",
    "            authors_to_keep[author].add(' '.join(author_to_check))\n",
    "            authors_to_keep_for_df.append( ' '.join(author_to_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_for_training = books_info[books_info['Authors_parsed'].isin(authors_to_keep_for_df)]\n",
    "book_id = books_for_training.iloc[1][\"Text#\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_author_list = []\n",
    "for author in authors_to_keep.keys():\n",
    "    for alternative in authors_to_keep[author]:\n",
    "        alternative_author_list.append([' '.join(author), alternative])\n",
    "alternatives = pd.DataFrame(alternative_author_list, columns=['Alternative_names', 'Authors_parsed'])\n",
    "books_for_training = pd.merge(books_for_training, alternatives, on='Authors_parsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mark Twain                    257\n",
       "Charles Dickens               207\n",
       "Arthur Conan Doyle            156\n",
       "Sir Richard Francis Burton     60\n",
       "Edith Nesbit                   42\n",
       "Emile Gaboriau                 34\n",
       "Booth Tarkington               33\n",
       "Name: Alternative_names, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_for_training['Alternative_names'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mark Twain                    47\n",
       "Charles Dickens               39\n",
       "Arthur Conan Doyle            38\n",
       "Sir Richard Francis Burton    15\n",
       "Edith Nesbit                   7\n",
       "Emile Gaboriau                 6\n",
       "Booth Tarkington               6\n",
       "Name: Alternative_names, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "_, books_for_training = train_test_split(books_for_training, test_size=.2, random_state=42)\n",
    "books_for_training['Alternative_names'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GUTENBERG_ROBOT_URL = \"http://www.gutenberg.org/robot/harvest?filetypes[]=txt\"\n",
    "r = requests.get(GUTENBERG_ROBOT_URL)\n",
    "GUTENBERG_MIRROR = re.search('(https?://[^/]+)[^\"]*.zip', r.text).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://aleph.gutenberg.org/7/70/70.zip',\n",
       " 'http://aleph.gutenberg.org/7/70/70-8.zip',\n",
       " 'http://aleph.gutenberg.org/7/70/70-0.zip']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gutenberg_text_urls(id, mirror=GUTENBERG_MIRROR, suffixes=(\"\", \"-8\", \"-0\")):\n",
    "    path = \"/\".join(id[:-1]) or \"0\"\n",
    "    return [f\"{mirror}/{path}/{id}/{id}{suffix}.zip\" for suffix in suffixes]\n",
    "\n",
    "gutenberg_text_urls(book_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import zipfile\n",
    "\n",
    "def download_gutenberg(id):\n",
    "    for url in gutenberg_text_urls(id):\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 404:\n",
    "            logging.warning(f\"404 for {url}\")\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        break\n",
    "    try:\n",
    "        z = zipfile.ZipFile(BytesIO(r.content))\n",
    "        print(f'success with {id}')\n",
    "    \n",
    "        if len(z.namelist()) != 1:\n",
    "            raise Exception(f\"Expected 1 file in {z.namelist()}\")\n",
    "    except:\n",
    "        print('file not a zip')\n",
    "        return\n",
    "        \n",
    "    return z.read(z.namelist()[0]).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:404 for http://aleph.gutenberg.org/7/70/70.zip\n",
      "WARNING:root:404 for http://aleph.gutenberg.org/7/70/70-8.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 70\n",
      "﻿The Project Gutenberg eBook of What Is Man? And Other Stories, by Mark Twain (Samuel Clemens)\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org. If you are not located in the United States, you\n",
      "will have to check the laws of the country where you are located before\n",
      "using this eBook.\n",
      "\n",
      "Title: What Is Man? And Other Stories\n",
      "\n",
      "Author: Mark Twain (Samuel Clemens)\n",
      "\n",
      "Release Date: June, 1993 [eBook #70]\n",
      "[Most recently updated: May 26, 2022]\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: UTF-8\n",
      "\n",
      "Produced by: An Anonymous Volunteer and David Widger\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK WHAT IS MAN? AND OTHER STORIES ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "WHAT IS MAN?\n",
      "AND OTHER ESSAYS\n",
      "\n",
      "By Mark Twain\n",
      "\n",
      "(Samuel Langhorne Clemens, 1835-1910)\n",
      "\n",
      "\n",
      "CONTENTS\n",
      "\n",
      " WHAT IS MAN?\n",
      " THE DEATH OF JEAN\n",
      " THE TURNING-POINT OF MY LIFE\n",
      " HOW TO MAKE HISTORY DATES STICK\n",
      " THE MEMORABLE ASSASSINATION\n",
      " A SCRAP OF CURIOUS HISTORY\n",
      " SWITZERLAND, THE CRADLE OF LIBERTY\n",
      " AT THE SHRINE OF ST. WAGNER\n",
      " WILLIAM DEAN HOWELLS\n",
      " ENGLISH AS SHE IS TAUGHT\n",
      " A SIMPLIFIED ALPHABET\n",
      " AS CONCERNS INTERPRETING THE DEITY\n",
      " CONCERNING TOBACCO\n",
      " THE BEE\n",
      " TAMING THE BICYCLE\n",
      " IS SHAKESPEARE DEAD?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "WHAT IS MAN?\n",
      "\n",
      "I\n",
      "\n",
      "a. Man the Machine. b. Personal Merit\n",
      "\n",
      "[The O\n"
     ]
    }
   ],
   "source": [
    "text = download_gutenberg(book_id)\n",
    "print(text[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading all the files\n",
    "Now we can download all the files in a simple loop; let’s create a simple function that gets and cleans the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUTENBERG_TEXT_URL = \"https://www.gutenberg.org/ebooks/{id}.txt.utf-8\"\n",
    "\n",
    "\n",
    "# def book_text(book_id):\n",
    "#     r = requests.get(GUTENBERG_TEXT_URL.format(id=book_id))\n",
    "#     text = r.text\n",
    "#     clean_text = strip_headers(text)\n",
    "#     return clean_text\n",
    "\n",
    "# data_path = Path(\"data/author_texts\")\n",
    "# data_path.mkdir(exist_ok=True)\n",
    "\n",
    "# count = 0\n",
    "# for idx, book in books_for_training.iterrows():\n",
    "#     if count%25 == 0:\n",
    "#         print(f'finished {count}/{books_for_training.shape[0]}')\n",
    "#     count += 1\n",
    "#     id = book[\"Text#\"]\n",
    "#     text = book_text(id)\n",
    "#     print(f\"Saving {book['Title']} by {book['Authors_parsed']} containing {len(text):_} characters\")\n",
    "#     with open(data_path / (id + \".txt\"), \"wt\") as f:\n",
    "#         f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(\"data/author_texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_book(document):\n",
    "    lines = document.split(\"\\n\")\n",
    "    start= 0\n",
    "    end = len(lines)\n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i]\n",
    "        if line.startswith(\"*** START OF THIS PROJECT GUTENBERG\"):\n",
    "            start = i + 1\n",
    "        elif line.startswith(\"*** END OF THIS PROJECT GUTENBERG\"):\n",
    "            end = i - 1\n",
    "    return \"\\n\".join(lines[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files: ['data/author_texts/8473.txt', 'data/author_texts/65044.txt', 'data/author_texts/922.txt', 'data/author_texts/50162.txt', 'data/author_texts/5785.txt', 'data/author_texts/59813.txt', 'data/author_texts/24026.txt', 'data/author_texts/61751.txt', 'data/author_texts/102.txt', 'data/author_texts/9021.txt', 'data/author_texts/3450.txt', 'data/author_texts/50361.txt', 'data/author_texts/11301.txt', 'data/author_texts/7154.txt', 'data/author_texts/9743.txt', 'data/author_texts/53254.txt', 'data/author_texts/65043.txt', 'data/author_texts/5838.txt', 'data/author_texts/66991.txt', 'data/author_texts/66952.txt', 'data/author_texts/675.txt', 'data/author_texts/18718.txt', 'data/author_texts/3441.txt', 'data/author_texts/8528.txt', 'data/author_texts/61193.txt', 'data/author_texts/66159.txt', 'data/author_texts/50164.txt', 'data/author_texts/7157.txt', 'data/author_texts/51252.txt', 'data/author_texts/17398.txt', 'data/author_texts/139.txt', 'data/author_texts/5813.txt', 'data/author_texts/37712.txt', 'data/author_texts/9557.txt', 'data/author_texts/3182.txt', 'data/author_texts/55091.txt', 'data/author_texts/9691.txt', 'data/author_texts/10446.txt', 'data/author_texts/35348.txt', 'data/author_texts/17314.txt', 'data/author_texts/52099.txt', 'data/author_texts/74248.txt', 'data/author_texts/564.txt', 'data/author_texts/3181.txt', 'data/author_texts/34652.txt', 'data/author_texts/809.txt', 'data/author_texts/5692.txt', 'data/author_texts/10135.txt', 'data/author_texts/17869.txt', 'data/author_texts/7245.txt', 'data/author_texts/9042.txt', 'data/author_texts/16021.txt', 'data/author_texts/52677.txt', 'data/author_texts/1422.txt', 'data/author_texts/58574.txt', 'data/author_texts/3180.txt', 'data/author_texts/3194.txt', 'data/author_texts/59336.txt', 'data/author_texts/3190.txt', 'data/author_texts/3184.txt', 'data/author_texts/824.txt', 'data/author_texts/70.txt', 'data/author_texts/45333.txt', 'data/author_texts/2852.txt', 'data/author_texts/48415.txt', 'data/author_texts/19841.txt', 'data/author_texts/19505.txt', 'data/author_texts/3178.txt', 'data/author_texts/2845.txt', 'data/author_texts/11588.txt', 'data/author_texts/62201.txt', 'data/author_texts/57721.txt', 'data/author_texts/15466.txt', 'data/author_texts/7104.txt', 'data/author_texts/2097.txt', 'data/author_texts/1415.txt', 'data/author_texts/8627.txt', 'data/author_texts/8633.txt', 'data/author_texts/1831.txt', 'data/author_texts/9706.txt', 'data/author_texts/1414.txt', 'data/author_texts/7105.txt', 'data/author_texts/52253.txt', 'data/author_texts/1748.txt', 'data/author_texts/14789.txt', 'data/author_texts/56586.txt', 'data/author_texts/20404.txt', 'data/author_texts/8618.txt', 'data/author_texts/8631.txt', 'data/author_texts/47529.txt', 'data/author_texts/9711.txt', 'data/author_texts/9739.txt', 'data/author_texts/7106.txt', 'data/author_texts/50109.txt', 'data/author_texts/6036.txt', 'data/author_texts/62140.txt', 'data/author_texts/2324.txt', 'data/author_texts/43207.txt', 'data/author_texts/1611.txt', 'data/author_texts/8621.txt', 'data/author_texts/1406.txt', 'data/author_texts/7103.txt', 'data/author_texts/4378.txt', 'data/author_texts/3198.txt', 'data/author_texts/3173.txt', 'data/author_texts/8583.txt', 'data/author_texts/8622.txt', 'data/author_texts/8623.txt', 'data/author_texts/3776.txt', 'data/author_texts/2326.txt', 'data/author_texts/3428.txt', 'data/author_texts/44620.txt', 'data/author_texts/70778.txt', 'data/author_texts/46285.txt', 'data/author_texts/37093.txt', 'data/author_texts/5837.txt', 'data/author_texts/4903.txt', 'data/author_texts/5836.txt', 'data/author_texts/5822.txt', 'data/author_texts/64384.txt', 'data/author_texts/25496.txt', 'data/author_texts/18717.txt', 'data/author_texts/20673.txt', 'data/author_texts/8719.txt', 'data/author_texts/917.txt', 'data/author_texts/730.txt', 'data/author_texts/30890.txt', 'data/author_texts/5798.txt', 'data/author_texts/74036.txt', 'data/author_texts/9029.txt', 'data/author_texts/28198.txt', 'data/author_texts/644.txt', 'data/author_texts/9558.txt', 'data/author_texts/59953.txt', 'data/author_texts/5148.txt', 'data/author_texts/69700.txt', 'data/author_texts/7159.txt', 'data/author_texts/69260.txt', 'data/author_texts/245.txt', 'data/author_texts/912.txt', 'data/author_texts/9010.txt', 'data/author_texts/1464.txt', 'data/author_texts/18506.txt', 'data/author_texts/38172.txt', 'data/author_texts/3070.txt', 'data/author_texts/8482.txt', 'data/author_texts/54109.txt', 'data/author_texts/5761.txt', 'data/author_texts/2350.txt', 'data/author_texts/2344.txt', 'data/author_texts/483.txt', 'data/author_texts/7200.txt', 'data/author_texts/2345.txt', 'data/author_texts/5760.txt', 'data/author_texts/42232.txt', 'data/author_texts/6533.txt', 'data/author_texts/69510.txt', 'data/author_texts/290.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "txt_files = glob.glob('data/author_texts/*.txt')\n",
    "print(\"Text files:\", txt_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import label encoder \n",
    "from sklearn import preprocessing \n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "books_for_training['Alternative_names_code']= label_encoder.fit_transform(books_for_training['Alternative_names']) \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158, 158)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_books_data(folder=data_folder):\n",
    "    documents = []\n",
    "    authors = []\n",
    "    subfolders = [subfolder for subfolder in glob.glob('data/author_texts/*.txt')]\n",
    "    for _, subfolder in enumerate(subfolders):\n",
    "        id = subfolder.split('/')[-1][:-4]\n",
    "        class_val = int(books_for_training[books_for_training['Text#']==id]['Alternative_names_code'])\n",
    "        with open(subfolder) as inf:\n",
    "            documents.append(clean_book(inf.read()))\n",
    "            authors.append(class_val)\n",
    "    return documents, np.array(authors, dtype='int')\n",
    "\n",
    "documents, classes = load_books_data(data_folder)\n",
    "len(documents), len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_words = [\"a\", \"able\", \"aboard\", \"about\", \"above\", \"absent\",\n",
    "                  \"according\" , \"accordingly\", \"across\", \"after\", \"against\",\n",
    "                  \"ahead\", \"albeit\", \"all\", \"along\", \"alongside\", \"although\",\n",
    "                  \"am\", \"amid\", \"amidst\", \"among\", \"amongst\", \"amount\", \"an\",\n",
    "                    \"and\", \"another\", \"anti\", \"any\", \"anybody\", \"anyone\",\n",
    "                    \"anything\", \"are\", \"around\", \"as\", \"aside\", \"astraddle\",\n",
    "                    \"astride\", \"at\", \"away\", \"bar\", \"barring\", \"be\", \"because\",\n",
    "                    \"been\", \"before\", \"behind\", \"being\", \"below\", \"beneath\",\n",
    "                    \"beside\", \"besides\", \"better\", \"between\", \"beyond\", \"bit\",\n",
    "                    \"both\", \"but\", \"by\", \"can\", \"certain\", \"circa\", \"close\",\n",
    "                    \"concerning\", \"consequently\", \"considering\", \"could\",\n",
    "                    \"couple\", \"dare\", \"deal\", \"despite\", \"down\", \"due\", \"during\",\n",
    "                    \"each\", \"eight\", \"eighth\", \"either\", \"enough\", \"every\",\n",
    "                    \"everybody\", \"everyone\", \"everything\", \"except\", \"excepting\",\n",
    "                    \"excluding\", \"failing\", \"few\", \"fewer\", \"fifth\", \"first\",\n",
    "                    \"five\", \"following\", \"for\", \"four\", \"fourth\", \"from\", \"front\",\n",
    "                    \"given\", \"good\", \"great\", \"had\", \"half\", \"have\", \"he\",\n",
    "                    \"heaps\", \"hence\", \"her\", \"hers\", \"herself\", \"him\", \"himself\",\n",
    "                    \"his\", \"however\", \"i\", \"if\", \"in\", \"including\", \"inside\",\n",
    "                    \"instead\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keeping\",\n",
    "                    \"lack\", \"less\", \"like\", \"little\", \"loads\", \"lots\", \"majority\",\n",
    "                    \"many\", \"masses\", \"may\", \"me\", \"might\", \"mine\", \"minority\",\n",
    "                    \"minus\", \"more\", \"most\", \"much\", \"must\", \"my\", \"myself\",\n",
    "                    \"near\", \"need\", \"neither\", \"nevertheless\", \"next\", \"nine\",\n",
    "                    \"ninth\", \"no\", \"nobody\", \"none\", \"nor\", \"nothing\",\n",
    "                    \"notwithstanding\", \"number\", \"numbers\", \"of\", \"off\", \"on\",\n",
    "                    \"once\", \"one\", \"onto\", \"opposite\", \"or\", \"other\", \"ought\",\n",
    "                    \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"part\",\n",
    "                    \"past\", \"pending\", \"per\", \"pertaining\", \"place\", \"plenty\",\n",
    "                    \"plethora\", \"plus\", \"quantities\", \"quantity\", \"quarter\",\n",
    "                    \"regarding\", \"remainder\", \"respecting\", \"rest\", \"round\",\n",
    "                    \"save\", \"saving\", \"second\", \"seven\", \"seventh\", \"several\",\n",
    "                    \"shall\", \"she\", \"should\", \"similar\", \"since\", \"six\", \"sixth\",\n",
    "                    \"so\", \"some\", \"somebody\", \"someone\", \"something\", \"spite\",\n",
    "                    \"such\", \"ten\", \"tenth\", \"than\", \"thanks\", \"that\", \"the\",\n",
    "                    \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\",\n",
    "                    \"therefore\", \"these\", \"they\", \"third\", \"this\", \"those\",\n",
    "                    \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\",\n",
    "                    \"till\", \"time\", \"to\", \"tons\", \"top\", \"toward\", \"towards\",\n",
    "                    \"two\", \"under\", \"underneath\", \"unless\", \"unlike\", \"until\",\n",
    "                    \"unto\", \"up\", \"upon\", \"us\", \"used\", \"various\", \"versus\",\n",
    "                    \"via\", \"view\", \"wanting\", \"was\", \"we\", \"were\", \"what\",\n",
    "                    \"whatever\", \"when\", \"whenever\", \"where\", \"whereas\",\n",
    "                    \"wherever\", \"whether\", \"which\", \"whichever\", \"while\",\n",
    "                    \"whilst\", \"who\", \"whoever\", \"whole\", \"whom\", \"whomever\",\n",
    "                    \"whose\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\",\n",
    "                    \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "extractor = CountVectorizer(vocabulary=function_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svr = SVC()\n",
    "grid = GridSearchCV(svr, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Pipeline([('feature_extraction', extractor),\n",
    "                      ('clf', grid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  UserWarning,\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer,f1_score\n",
    "\n",
    "f1_ = make_scorer(f1_score, average='weighted')\n",
    "scores = cross_val_score(pipeline1, documents, classes, scoring=f1_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5066009257894807\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  UserWarning,\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.632\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('feature_extraction',\n",
    "                      CountVectorizer(analyzer='char', ngram_range=(3, 3))),\n",
    "                      ('classifier', grid)])\n",
    "scores = cross_val_score(pipeline, documents, classes, scoring=f1_)\n",
    "print(\"Score: {:.3f}\".format(np.mean(scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

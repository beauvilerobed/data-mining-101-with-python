{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommending Movies Using Affinity Analysis\n",
    "\n",
    "Here, we will look at affinity analysis that determines when objects occur frequently together. This is colloquially called market basket analysis, after one of the use cases of determining when items are purchased together frequently.\n",
    "\n",
    "The key concepts of this chapter are as follows:\n",
    "- Affinity analysis\n",
    "- Feature association mining using the Apriori algorithm\n",
    "- Movie recommendations\n",
    "- Sparse data formats\n",
    "\n",
    "## Affinity analysis\n",
    "\n",
    "Affinity analysis is the task of determining when objects are used in similar ways. In the previous section, we focused on whether the objects themselves are similar. The data for affinity analysis is often described in the form of a **transaction**. Intuitively, this comes from a transaction at a storeâ€”determining when objects are purchased together. \n",
    "\n",
    "However, it can be applied to many processes:\n",
    "- Fraud detection\n",
    "- Customer segmentation\n",
    "- Software optimization\n",
    "- Product recommendations\n",
    "\n",
    "Affinity analysis is usually much more exploratory than classification. We often don't have the complete dataset we expect for many classification tasks. For instance, in movie recommendation, we have reviews from different people on different movies. However, it is unlikely we have each reviewer review all of the movies in our dataset. This leaves an important and difficult question in affinity analysis. If a reviewer hasn't reviewed a movie, is that an indication that they aren't interested in the movie (and therefore wouldn't recommend it) or simply that they haven't reviewed it yet?\n",
    "\n",
    "## Algorithms for affinity analysis\n",
    "\n",
    "With a naive rule creation, such as our previous algorithm, the growth in time needed to compute these rules increases exponentially. As we add more items, the time it takes to compute all rules increases significantly faster. Specifically, the total possible number of rules is 2n - 1. For our five-item dataset, there are 31 possible rules. For 10 items, it is 1023. For just 100 items, the number has 30 digits. Even the drastic increase in computing power couldn't possibly keep up with the increases in the number of items stored online. Therefore, we need algorithms that work smarter, as opposed to computers that work harder.\n",
    "\n",
    "The classic algorithm for affinity analysis is called the **Apriori algorithm**. It addresses the exponential problem of creating sets of items that occur frequently within a database, called **frequent itemsets**. Once these frequent itemsets are discovered, creating association rules is straightforward.\n",
    "\n",
    "The intuition behind Apriori is both simple and clever. First, we ensure that a rule has sufficient support within the dataset. Defining a minimum support level is the key parameter for Apriori. To build a frequent itemset, for an itemset (A, B) to have a support of at least 30, both A and B must occur at least 30 times in the database. This property extends to larger sets as well. For an itemset (A, B, C, D) to be considered\n",
    "frequent, the set (A, B, C) must also be frequent (as must D).\n",
    "\n",
    "These frequent itemsets can be built up and possible itemsets that are not frequent (of which there are many) will never be tested. This saves significant time in testing new rules.\n",
    "\n",
    "Other example algorithms for affinity analysis include the **Eclat** and **FP-growth** algorithms. There are many improvements to these algorithms in the data mining literature that further improve the efficiency of the method.\n",
    "\n",
    "## Choosing parameters\n",
    "\n",
    "To perform association rule mining for affinity analysis, we first use the Apriori to generate frequent itemsets. Next, we create association rules (for example, if a person recommended movie X, they would also recommend movie Y) by testing combinations of premises and conclusions within those frequent itemsets.\n",
    "\n",
    "For the first stage, the Apriori algorithm needs a value for the minimum support that an itemset needs to be considered frequent. Any itemsets with less support will not be considered. Setting this minimum support too low will cause Apriori to test a larger number of itemsets, slowing the algorithm down. Setting it too high will result in fewer itemsets being considered frequent.\n",
    "\n",
    "In the second stage, after the frequent itemsets have been discovered, association\n",
    "rules are tested based on their confidence. We could choose a minimum confidence\n",
    "level, a number of rules to return, or simply return all of them and let the user decide\n",
    "what to do with them.\n",
    "\n",
    "In this chapter, we will return only rules above a given confidence level. Therefore,\n",
    "we need to set our minimum confidence level. Setting this too low will result in rules\n",
    "that have a high support, but are not very accurate. Setting this higher will result in\n",
    "only more accurate rules being returned, but with fewer rules being discovered.\n",
    "\n",
    "## The movie recommendation problem\n",
    "\n",
    "Product recommendation is big business. Online stores use it to up-sell to\n",
    "customers by recommending other products that they could buy. Making better\n",
    "recommendations leads to better sales. When online shopping is selling to millions\n",
    "of customers every year, there is a lot of potential money to be made by selling more\n",
    "items to these customers.\n",
    "\n",
    "Product recommendations have been researched for many years; however, the field\n",
    "gained a significant boost when Netflix ran their Netflix Prize between 2007 and\n",
    "2009. This competition aimed to determine if anyone can predict a user's rating of a\n",
    "film better than Netflix was currently doing. The prize went to a team that was just\n",
    "over 10 percent better than the current solution. While this may not seem like a large\n",
    "improvement, such an improvement would net millions to Netflix in revenue from\n",
    "better movie recommendations.\n",
    "\n",
    "## Obtaining the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Features with Transformers\n",
    "\n",
    "The datasets we have used so far have been described in terms of features. In the previous notebooks, we used a transaction-centric dataset. However, ultimately this was just a different format for representing feature-based data.\n",
    "\n",
    "There are many other types of datasets, including text, images, sounds, movies, or even real objects. Most data mining algorithms, however, rely on having numerical or categorical features. This means we need a way to represent these types before we input them into the data mining algorithm.\n",
    "\n",
    "The key concepts:\n",
    "- Extracting features from datasets\n",
    "- Creating new features\n",
    "- Selecting good features\n",
    "- Creating your own transformer for custom datasets\n",
    "\n",
    "## Feature extraction\n",
    "\n",
    "Extracting features is one of the most critical tasks in data mining, and **it generally affects your end result more than the choice of data mining algorithm**. Unfortunately, there are no hard and fast rules for choosing features that will result in high performance data mining. In many ways, this is where the science of data mining becomes more of an art. Creating good features relies on intuition, domain expertise, data mining experience, trial and error, and sometimes a little luck.\n",
    "\n",
    "## Representing reality in models\n",
    "\n",
    "Not all datasets are presented in terms of features. Sometimes, a dataset consists of nothing more than all of the books that have been written by a given author. Sometimes, it is the film of each of the movies released in 1979. At other times, it is a library collection of interesting historical artifacts.\n",
    "\n",
    "From these datasets, we may want to perform a data mining task. For the books, we may want to know the different categories that the author writes. In the films, we may wish to see how women are portrayed. In the historical artifacts, we may wantto know whether they are from one country or another. It isn't possible to just pass these raw datasets into a decision tree and see what the result is.\n",
    "\n",
    "For a data mining algorithm to assist us here, we need to represent these as features. Features are a way to create a model and the model provides an *approximation* of reality in a way that data mining algorithms can understand. Therefore, **a model is just a simplified version of some aspect of the real world**. As an example, the game of chess is a simplified model for historical warfare.\n",
    "\n",
    "Selecting features has another advantage: they reduce the complexity of the real world into a more manageable model. Imagine how much information it would take to properly, accurately, and fully describe a real-world object to someone that has no background knowledge of the item. You would need to describe the size, weight,\n",
    "texture, composition, age, flaws, purpose, origin, and so on.\n",
    "\n",
    "The complexity of real objects is too much for current algorithms, so we use these simpler models instead.\n",
    "\n",
    "However, there is a downside as this simplification reduces the detail, or may remove good indicators of the things we wish to perform data mining on.\n",
    "\n",
    "Thought should always be given to how to represent reality in the form of a model. Rather than just using what has been used in the past, you need to consider the goal of the data mining exercise. What are you trying to achieve?\n",
    "\n",
    "**Note**: Not all features need to be numeric or categorical. Algorithms have been developed that work directly on text, graphs, and other data structures. Unfortunately, those algorithms are outside the scope of\n",
    "this notebook. In this book, we mainly use numeric or categorical features.\n",
    "\n",
    "The Adult dataset is a great example of taking a complex reality and attempting to model it using features. In this dataset, the aim is to estimate if someone earns more than $50,000 per year. To download the dataset, navigate to https://archive.ics.uci.edu/ml/datasets/Adult and click on the Data Folder link. Download the\n",
    "adult.data and adult.names into a directory named Adult in your data folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA = 'data/adult/'\n",
    "ADULT = DATA + 'adult.data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = pd.read_csv(ADULT, header=None, names=[\"Age\", \"Work-Class\", \"fnlwgt\", \"Education\",\n",
    "                                                        \"Education-Num\", \"Marital-Status\", \"Occupation\",\n",
    "                                                        \"Relationship\", \"Race\", \"Sex\", \"Capital-gain\",\n",
    "                                                        \"Capital-loss\", \"Hours-per-week\", \"Native-Country\",\n",
    "                                                        \"Earnings-Raw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Work-Class', 'fnlwgt', 'Education', 'Education-Num',\n",
       "       'Marital-Status', 'Occupation', 'Relationship', 'Race', 'Sex',\n",
       "       'Capital-gain', 'Capital-loss', 'Hours-per-week', 'Native-Country',\n",
       "       'Earnings-Raw'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult.dropna(how='all', inplace=True)\n",
    "adult.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common feature patterns\n",
    "\n",
    "While there are millions of ways to create features, there are some common patterns that are employed across different disciplines. However, choosing appropriate features is tricky and it is worth considering how a feature might correlate to the end result. As the adage says, don't judge a book by its coverâ€”it is probably not worth considering the size of a book if you are interested in the message contained within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[\"Hours-per-week\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[\"Education-Num\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[\"Work-Class\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.arange(30).reshape((10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:,1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt = VarianceThreshold()\n",
    "Xt = vt.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vt.variances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = adult[[\"Age\", \"Education-Num\", \"Capital-gain\", \"Capital-loss\", \"Hours-per-week\"]].values\n",
    "y = (adult[\"Earnings-Raw\"] == ' >50K').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "transformer = SelectKBest(score_func=chi2, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_chi2 = transformer.fit_transform(X, y)\n",
    "print(transformer.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def multivariate_pearsonr(X, y):\n",
    "    scores, pvalues = [], []\n",
    "    for column in range(X.shape[1]):\n",
    "        cur_score, cur_p = pearsonr(X[:,column], y)\n",
    "        scores.append(abs(cur_score))\n",
    "        pvalues.append(cur_p)\n",
    "    return (np.array(scores), np.array(pvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = SelectKBest(score_func=multivariate_pearsonr, k=3)\n",
    "Xt_pearson = transformer.fit_transform(X, y)\n",
    "print(transformer.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = DecisionTreeClassifier(random_state=14)\n",
    "scores_chi2 = cross_val_score(clf, Xt_chi2, y, scoring='accuracy')\n",
    "scores_pearson = cross_val_score(clf, Xt_pearson, y, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Chi2 performance: {0:.3f}\".format(scores_chi2.mean()))\n",
    "print(\"Pearson performance: {0:.3f}\".format(scores_pearson.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.utils import as_float_array\n",
    "\n",
    "class MeanDiscrete(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        X = as_float_array(X)\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = as_float_array(X)\n",
    "        assert X.shape[1] == self.mean.shape[0]\n",
    "        return X > self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_discrete = MeanDiscrete()\n",
    "X_mean = mean_discrete.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file adult_tests.py\n",
    "import numpy as np\n",
    "from numpy.testing import assert_array_equal\n",
    "\n",
    "def test_meandiscrete():\n",
    "    X_test = np.array([[ 0,  2],\n",
    "                        [ 3,  5],\n",
    "                        [ 6,  8],\n",
    "                        [ 9, 11],\n",
    "                        [12, 14],\n",
    "                        [15, 17],\n",
    "                        [18, 20],\n",
    "                        [21, 23],\n",
    "                        [24, 26],\n",
    "                        [27, 29]])\n",
    "    mean_discrete = MeanDiscrete()\n",
    "    mean_discrete.fit(X_test)\n",
    "    assert_array_equal(mean_discrete.mean, np.array([13.5, 15.5]))\n",
    "    X_transformed = mean_discrete.transform(X_test)\n",
    "    X_expected = np.array([[ 0,  0],\n",
    "                            [ 0, 0],\n",
    "                            [ 0, 0],\n",
    "                            [ 0, 0],\n",
    "                            [ 0, 0],\n",
    "                            [ 1, 1],\n",
    "                            [ 1, 1],\n",
    "                            [ 1, 1],\n",
    "                            [ 1, 1],\n",
    "                            [ 1, 1]])\n",
    "    assert_array_equal(X_transformed, X_expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_meandiscrete()\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('mean_discrete', MeanDiscrete()),\n",
    "                     ('classifier', DecisionTreeClassifier(random_state=14))])\n",
    "scores_mean_discrete = cross_val_score(pipeline, X, y, scoring='accuracy')\n",
    "print(\"Mean Discrete performance: {0:.3f}\".format(scores_mean_discrete.mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

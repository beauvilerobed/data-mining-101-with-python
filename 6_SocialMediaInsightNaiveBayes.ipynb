{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Insight Using Naive Bayes\n",
    "\n",
    "\n",
    "Text-based datasets contain a lot of information, whether they are books, historical documents, social media, e-mail, or any of the other ways we communicate via writing. Extracting features from text-based datasets and using them for classification is a difficult problem. There are, however, some common patterns for text mining. We look at disambiguating terms in social media using the Naive Bayes algorithm, which is a powerful and surprisingly simple algorithm. Naive Bayes takes a few shortcuts to properly compute the probabilities for classification, hence the term naive in the name. It can also be extended to other types of datasets quite easily and doesn't rely on numerical features. The model is a baseline for text mining studies, as the process can work reasonably well for a variety of datasets.\n",
    "\n",
    "We will cover the following topics in this chapter:\n",
    "- Downloading data from social network APIs\n",
    "- Transformers for text\n",
    "- Naive Bayes classifier\n",
    "- Using JSON for saving and loading datasets\n",
    "- The NLTK library for extracting features from text\n",
    "- The F-measure for evaluation\n",
    "\n",
    "## Disambiguation\n",
    "\n",
    "Text is often called an **unstructured** format. There is a lot of information there, but it is just there; no headings, no required format, loose syntax and other problems prohibit the easy extraction of information from text. The data is also highly connected, with lots of mentions and cross-references—just not in a format that allows us to easily extract it!\n",
    "\n",
    "We can compare the information stored in a book with that stored in a large database to see the difference. In the book, there are characters, themes, places, and lots of information. However, the book needs to be read and, more importantly, interpreted to gain this information. The database sits on your server with column names and data types. All the information is there and the level of interpretation needed is quite low. Information about the data, such as its type or meaning is called **metadata**, and text lacks it. A book also contains some metadata in the form of a table of contents and index but the degree is significantly lower than that of a database. \n",
    "\n",
    "One of the problems is the term **disambiguation**. When a person uses the word bank, is this a financial message or an environmental message (such as river bank)? This type of disambiguation is quite easy in many circumstances for humans (although there are still troubles), but much harder for computers to do.\n",
    "\n",
    "Here, we will look at disambiguating the use of the term Python on Twitter's stream. A message on Twitter is called a tweet and is limited to 140 characters. This means there is little room for context. There isn't much metadata available although hashtags are often used to denote the topic of the tweet.\n",
    "\n",
    "When people talk about Python, they could be talking about the following things:\n",
    "- The programming language Python\n",
    "- Monty Python, the classic comedy group\n",
    "- The snake Python\n",
    "- A make of shoe called Python\n",
    "\n",
    "There can be many other things called Python. The aim of our experiment is to take a tweet mentioning Python and determine whether it is talking about the programming language, based only on the content of the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data from a social network\n",
    "\n",
    "We are going to download a corpus of data from Twitter and use it to sort out spam from useful content. Twitter provides a robust API for collecting information from its servers and this API is free for small-scale usage. It is, however, subject to some conditions that you'll need to be aware of if you start using Twitter's data in a commercial setting.\n",
    "\n",
    "First, you'll need to sign up for a Twitter account (which is free). Go to http://twitter.com and register an account if you do not already have one.\n",
    "\n",
    "Next, you'll need to ensure that you only make a certain number of requests per minute. This limit is currently 180 requests per hour. It can be tricky ensuring that you don't breach this limit, so it is highly recommended that you use a library to talk to Twitter's API.\n",
    "\n",
    "You will need a key to access Twitter's data. Go to http://twitter.com and sign in to your account.\n",
    "\n",
    "When you are logged in, go to https://apps.twitter.com/ and click on Create New App.\n",
    "\n",
    "Create a name and description for your app, along with a website address.\n",
    "If you don't have a website to use, insert a placeholder. Leave the Callback URL field\n",
    "blank for this app—we won't need it. Agree to the terms of use (if you do)\n",
    "and click on Create your Twitter application.\n",
    "\n",
    "Keep the resulting website open—you'll need the access keys that are on this page.\n",
    "Next, we need a library to talk to Twitter. There are many options; the one I like is\n",
    "simply called twitter, and is the official Twitter Python library.\n",
    "\n",
    "Note: You can install twitter using pip3 install twitter if you are\n",
    "using pip to install your packages. If you are using another system,\n",
    "check the documentation at https://github.com/sixohsix/\n",
    "twitter.\n",
    "\n",
    "Create a new IPython Notebook to download the data. We will create several\n",
    "notebooks in this chapter for various different purposes, so it might be a good idea to\n",
    "also create a folder to keep track of them. This first notebook, ch6_get_twitter, is\n",
    "specifically for downloading new Twitter data.\n",
    "\n",
    "First, we import the twitter library and set our authorization tokens. The consumer\n",
    "key, consumer secret will be available on the Keys and Access Tokens tab on your\n",
    "Twitter app's page. To get the access tokens, you'll need to click on the Create my\n",
    "access token button, which is on the same page. Enter the keys into the appropriate\n",
    "places in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling the class values for the twitter dataset.\n",
    "import os\n",
    "input_filename = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"twitter\", \"python_tweets.json\")\n",
    "classes_filename = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"twitter\", \"python_classes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "tweets = []\n",
    "with open(input_filename) as inf:\n",
    "    for line in inf:\n",
    "        if len(line.strip()) == 0:\n",
    "            continue\n",
    "        tweets.append(json.loads(line)['text'])\n",
    "print(\"Loaded {} tweets\".format(len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(classes_filename) as inf:\n",
    "    labels = json.load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = min(len(tweets), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets = [t.lower() for t in tweets[:n_samples]]\n",
    "labels = labels[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_true = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:.1f}% have class 1\".format(np.mean(y_true == 1) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from nltk import word_tokenize\n",
    "\n",
    "class NLTKBOW(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [{word: True for word in word_tokenize(document)}\n",
    "                 for document in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline = Pipeline([('bag-of-words', NLTKBOW()),\n",
    "                     ('vectorizer', DictVectorizer()),\n",
    "                     ('naive-bayes', BernoulliNB())\n",
    "                     ])\n",
    "scores = cross_val_score(pipeline, sample_tweets, y_true, cv=10, scoring='f1')\n",
    "print(\"Score: {:.3f}\".format(np.mean(scores)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
